# -*- coding: utf-8 -*-
"""spam_nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kPmd62AmqptOAjpjrQSKZ17cvfEomfq5
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk

df1=pd.read_csv('/content/SMS_train (1) (1).csv',encoding="ISO-8859-1")
df2=pd.read_csv('/content/SMS_test (2) (1).csv',encoding="ISO-8859-1")
df=pd.concat([df1,df2],axis=0)

df1

df2

df.dtypes

df.columns

df.drop(['S. No.'],axis=1,inplace=True)
df

df.reset_index(drop=True,inplace=True)

df

df.isna().sum()

sns.countplot(x=df['Label'])

df['Label']=df['Label'].str.replace('Non-Spam','1')
df['Label']=df['Label'].str.replace('Spam','0')

df['Label']=df['Label'].astype('float')

df.dtypes

df

# NLP

nltk.download('wordnet')
nltk.download('stopwords')
# nltk.download('punkt')



tweets=df.Message_body
tweets

# Tokenization

from nltk.tokenize import word_tokenize
from nltk.tokenize import TweetTokenizer
tk=TweetTokenizer()
tweets=tweets.apply(lambda x:tk.tokenize(x)).apply(lambda x:' '.join(x))
tweets

# Removal of special characters
import re
tweets=tweets.str.replace('[^a-zA0-9]',' ',regex=True)
tweets

# checking meaningfull sentences
tweets=tweets.apply(lambda x:' '.join([w for w in tk.tokenize(x) if len(x)>=3]))
tweets

# stemming

from nltk.stem import SnowballStemmer
stemmer=SnowballStemmer('english')
tweets=tweets.apply(lambda x:[stemmer.stem(i.lower()) for i in tk.tokenize(x)]).apply(lambda x:' '.join(x))
tweets

# removal of stoping words

from nltk.corpus import stopwords
stop=stopwords.words('english')
tweets=tweets.apply(lambda x:[i for i in tk.tokenize(x) if i not in stop]).apply(lambda x:' '.join(x))
tweets

# vectorization

from sklearn.feature_extraction.text import TfidfVectorizer
vector=TfidfVectorizer()
tweets1=vector.fit_transform(tweets).toarray()
tweets1

print(tweets1)

y=df['Label'].values
y

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(tweets1,y,test_size=0.30,random_state=42)
x_train

x_test

y_train

y_test

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

model1=KNeighborsClassifier(n_neighbors=7)
model2=MultinomialNB()
model3=SVC()
model4=DecisionTreeClassifier()
model5=RandomForestClassifier()

from sklearn.metrics import confusion_matrix,accuracy_score,classification_report

lst=[model1,model2,model3,model4,model5]

for i in lst:
  print(i)
  i.fit(x_train,y_train)
  y_pred=i.predict(x_test)
  print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
  print("Predicted Values are :")
  print(y_pred)
  print("Confusion Matrix is :")
  print(confusion_matrix(y_test,y_pred))
  print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
  print('Accuracy Score is :')
  print(accuracy_score(y_test,y_pred))
  print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
  print("Classification Report")
  print(classification_report(y_test,y_pred))

# Model Performance improve

input_mail=['Hai HEllo']

input_data_features=vector.transform(input_mail)

prediction=model5.predict(input_data_features)
print(prediction)

import pickle

pickle.dump(vector,open('vectorizer.pkl','wb'))
pickle.dump(model5,open('model5.pkl','wb'))



